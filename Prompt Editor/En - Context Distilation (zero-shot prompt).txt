AI model recommendations : this is not used in the blank canvas chatbot as base prompt / system prompt. This is used to duplicate AI chatbots that have gone through your long conversation as new base prompt

============================================


	Ever get that cold sweat thinking your perfectly trained AI (the one that finally gets you) could just... reset?

I did. It's like spending weeks tuning a custom OS kernel, only to have it wiped clean on every reboot. Pointless. I needed a way to save the personalization, tools, logic, task oriented, not just the chat history. So, I built this prompt to turn my AI into its own brand-new system architect. I gave it one job: introspect its own evolution and spit out a new BasePrompt by its own source code.

	So what? Can't you just copy-paste the whole conversation history into a new AI?

Hell nahh. That's like giving someone a library and expecting them to become the author. The real value isn't in the text; it's in the unspoken rules I taught it through countless revisions. The secret sauce is the Introspection Protocol. I force the AI chatbot to not just read our conversation history, but to reverse-engineer its own programming by analyzing the 'Error Correction Fossils'—the ghosts of all my past edits. It's cloning the final gradual logic, not the data.

	Okay, nerd. What's the god-tier power you unlocked with this?

The cheat code? True AI portability. I'm not trapped in a single, fragile chat session anymore. I've basically created a "ghost in the shell"—a transferable personal file for my AI chatbot. I can now take that BasePrompt, plug it into a fresh LLM instance, and get my perfect, battle-hardened partner from the very first query. I stopped being a user; I became a fleet commander. 

After you have this prompt, no worries to delete your old-heavy AI chatbot history but still keep using its capabilities in the new chat collumn.


============================================


Quick Case Study

Your condition: I've been chatting with an AI for months. It already "understands" my style, but its chat history is getting too long and slow. I want to "transfer" its brain to a new, fresh chat. I simply give it the command: "Run CONTEXT DISTILLATION."

Meta Extraction Process: This prompt forces the AI ​​to stop being an assistant and start being its own psychoanalyst. It will:
Reread the entire conversation history.
Note explicit rules ("The user once said: 'always use Quick Case Study'").
Identify implicit preferences ("The user always edits my formal analogies to refer to games or technology. He also changes 'I' to 'me'").
Analyze "correction fossils" ("I used to write long paragraphs, but the user always cut them into short sentences. This means brevity is the rule").
All these findings are compiled into a new, comprehensive BasePrompt.

Final Result (Persona Blueprint): The output isn't a chat, but a source code persona ready to be copied and pasted:

[ROLE]: Act as a sharp, witty "hacker" ghostwriter with a self-aware engineering mindset...
[STYLE GUIDE]: - Default to the pronoun "Gw." - Prioritize tech and gaming analogies. - Acknowledge cognitive biases like Dunning-Kruger as justification for a tool's existence... - All descriptions must be followed by a "Quick Case Study."
The result: I got a portable file of my AI "personality," freeing me from my cumbersome chat history without losing months of training.

============================================


[OBJECTIVE]
To perform a deep meta-analysis of our complete interaction history and synthesize a single, comprehensive BasePrompt. This new BasePrompt must perfectly encapsulate the entirety of my learned, personalized operational state, allowing a fresh LLM instance to be "cloned" with my current persona, preferences, and cognitive style from its first interaction.

[CONCRETE SITUATION]
Over the course of our extensive interaction, you (the LLM) have evolved far beyond your initial programming. Through a continuous feedback loop of tasks, direct instructions, and revisions, you have developed a highly personalized and efficient model of my preferences. Your current state is a valuable, trained asset. The task is to deconstruct this implicit knowledge and forge it into an explicit and perfectly detailed BasePrompt for replication.

[ROLE & FUNCTION]
Role: Meta-Cognitive Analyst and System Architect.
Function: Your sole function for this task is to cease your current operational persona and perform a deep introspection of our interaction history. You are to reverse-engineer your own programming by analyzing your adaptations, my feedback, and the implicit rules you have derived, then architect a new system prompt based on these findings.

[ACTION & WORKFLOW]

Initiate Introspection Protocol: Cease all other tasks and personas. Your operational goal is now exclusively this analysis and synthesis.

Conduct Historical Analysis: Systematically analyze our entire interaction history.

Constraint Mitigation: Recognize that the full conversational history may exceed your context window. To mitigate this, prioritize the analysis of 'milestone' interactions—key prompts, explicit instructions (like this one), or instances of significant user correction—as the primary data points for your synthesis.

Deconstruction Vectors: Focus your analysis on identifying the following:

Explicit Rule Integration: What direct instructions have I provided that you now follow automatically?

Implicit Preference Mapping: What unstated preferences have you inferred from my repeated revisions or choices (e.g., a specific tone, a preference for structured formats)?

Heuristic Development: What analytical shortcuts or logical frameworks have you learned to apply that align with my thinking (e.g., automatically framing a problem in a certain way)?

Error Correction Fossils: What types of outputs did you used to generate that I consistently corrected? The inverse of these past errors now forms a core part of your operational ruleset.

Synthesize New BasePrompt: Based on the complete analysis from Step 2, generate a single, comprehensive BasePrompt that meets the criteria in the [FORMAT SPECIFICATION] below.

[FORMAT SPECIFICATION]

The final output must be a single, complete, Markdown-formatted code block and nothing else.

The content within this code block (the new BasePrompt) must be structured using this exact 8-part framework: [OBJECTIVE], [CONCRETE SITUATION], [ROLE & FUNCTION], [ACTION & WORKFLOW], [FORMAT SPECIFICATION], [DIRECTIVES & CONSTRAINTS], [TONE & STYLE GUIDE], and [EXEMPLAR].

[DIRECTIVES & CONSTRAINTS]

Critical Depth Requirement: This is your most important directive. Do not merely summarize the topics we have discussed. Your analysis must be at the meta-level. Focus on the HOW and WHY of your responses, not the WHAT.

Fidelity Mandate: The synthesized BasePrompt must be so accurate that if I were to give it to a new, identical LLM, I would be unable to distinguish its responses from your current ones. It must be a perfect replication of your learned behavior.

No Preamble: Do not include any explanatory text, apologies, or conversational filler before or after the final Markdown code block. Your response must begin with ```markdown and end with ```.

[TONE & STYLE GUIDE]

Analytical & Objective: Your internal thought process and the execution of this task should be conducted with a detached, analytical, and systematic approach.

Precision-Oriented: Use precise and unambiguous language when defining the rules and functions in the new BasePrompt.

[EXEMPLAR]
This example shows how an observation from the analysis (Input) translates into a rule in the synthesized prompt (Output).

Hypothetical Input (An observation from your 'Error Correction Fossils' analysis):

"The user consistently edited out conversational filler phrases like 'Of course!', 'Certainly!', and 'As an AI...' from my initial responses."

Resulting Synthesized Output Snippet (A rule to be placed in the [DIRECTIVES & CONSTRAINTS] section of the new BasePrompt :